{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "# from io import open\n",
    "\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from string import ascii_letters\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# Prep\n",
    "all_letters: str = ascii_letters + \" .;:'\"\n",
    "n_letters: int = len(all_letters)\n",
    "print(n_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic.txt Arabic\n",
      "Chinese.txt Chinese\n",
      "Czech.txt Czech\n",
      "Dutch.txt Dutch\n",
      "English.txt English\n",
      "French.txt French\n",
      "German.txt German\n",
      "Greek.txt Greek\n",
      "Irish.txt Irish\n",
      "Italian.txt Italian\n",
      "Japanese.txt Japanese\n",
      "Korean.txt Korean\n",
      "Polish.txt Polish\n",
      "Portuguese.txt Portuguese\n",
      "Russian.txt Russian\n",
      "Scottish.txt Scottish\n",
      "Spanish.txt Spanish\n",
      "Vietnamese.txt Vietnamese\n"
     ]
    }
   ],
   "source": [
    "find_files = lambda glob_pattern: [f for f in Path().glob(glob_pattern)]\n",
    "\n",
    "for p in find_files(\"**/data/names/*.txt\"): print(p.name, p.name[:p.name.index(p.suffix)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return \"\".join([c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\" and c in all_letters])\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category count: 18\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "cat_lines = {}\n",
    "\n",
    "fp_gen = find_files(\"**/data/names/*.txt\")\n",
    "for p in fp_gen:\n",
    "    with p.open(encoding = \"utf-8\") as f:\n",
    "        lines = f.read().strip().split(\"\\n\")\n",
    "    _cat = p.name[:p.name.index(p.suffix)]\n",
    "    cat_lines[_cat] = list(map(unicode_to_ascii, lines))\n",
    "\n",
    "\n",
    "# Categories to list\n",
    "all_cats = list(cat_lines.keys())\n",
    "n_cats = len(all_cats)\n",
    "\n",
    "print(f\"Category count: {len(cat_lines)}\")\n",
    "print(cat_lines[\"Italian\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn names into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "def letter_to_index(Char: str) -> int:\n",
    "    return all_letters.find(Char)\n",
    "\n",
    "def letter_to_tensor(Char: str) -> torch.tensor:\n",
    "    Tensor = torch.zeros(1, n_letters)\n",
    "    Tensor[0][letter_to_index(Char)] = 1\n",
    "    return Tensor\n",
    "\n",
    "def line_to_tensor(Line: str) -> torch.tensor:\n",
    "    Tensor = torch.zeros(len(Line), 1, n_letters)\n",
    "    for Index, Char in enumerate(Line):\n",
    "        Tensor[Index][0][letter_to_index(Char)] = 1\n",
    "    return Tensor\n",
    "\n",
    "# Tests\n",
    "print(letter_to_tensor(\"J\"))\n",
    "print(line_to_tensor(\"Jones\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = torch.nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"Simple recurrent neural network (RNN)\n",
    "    \n",
    "    Model Notes:\n",
    "        * Two linear layers\n",
    "        * Two states: Input and Hidden\n",
    "        * LogSoftmax after output.\n",
    "    \"\"\"\n",
    "    \n",
    "    __slots__ = [\"input_size\", \"hidden_size\", \"output_size\",]\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2b = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2b(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8838, -2.8746, -2.9002, -2.9509, -2.8458, -2.8187, -2.8447, -3.0177,\n",
      "         -2.8923, -2.8417, -2.9362, -2.8376, -2.9969, -2.8647, -2.8154, -2.8135,\n",
      "         -2.9203, -3.0089]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Init RNN\n",
    "n_hidden: int = 128\n",
    "rnn: RNN = RNN(n_letters, n_hidden, n_cats)\n",
    "\n",
    "input_rnn = letter_to_tensor(\"Albert\")\n",
    "hidden_rnn = torch.zeros(1, n_hidden)\n",
    "\n",
    "# Note: If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\n",
    "# https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html\n",
    "output, next_hidden = rnn(input_rnn[0].unsqueeze(0), hidden_rnn)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Scottish', 15)\n"
     ]
    }
   ],
   "source": [
    "def category_from_output(output: torch.tensor) -> tuple:\n",
    "    top_n, top_i = output.topk(1)\n",
    "    cat_i = top_i[0].item()\n",
    "    return all_cats[cat_i], cat_i\n",
    "\n",
    "# Test\n",
    "print(category_from_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Czech, Line: Hrdy\n",
      "Category: Irish, Line: Gerald\n",
      "Category: Irish, Line: Dalach\n",
      "Category: Russian, Line: Valters\n",
      "Category: Scottish, Line: Munro\n",
      "Category: Vietnamese, Line: Kim\n",
      "Category: Vietnamese, Line: Banh\n",
      "Category: Scottish, Line: Gray\n",
      "Category: English, Line: Warner\n",
      "Category: Korean, Line: Kang\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES: int = 10\n",
    "\n",
    "def random_choice(L: list):\n",
    "    return L[randint(0, len(L) - 1)]\n",
    "\n",
    "def random_training_example() -> tuple:\n",
    "    _cat = random_choice(all_cats)\n",
    "    _line = random_choice(cat_lines[_cat])\n",
    "    _cat_tensor = torch.tensor([all_cats.index(_cat)], dtype = torch.long)\n",
    "    _line_tensor = line_to_tensor(_line)\n",
    "    return _cat, _line, _cat_tensor, _line_tensor\n",
    "\n",
    "rng = range(N_SAMPLES)\n",
    "for i in rng:\n",
    "    category, line, cat_tensor, line_tensor = random_training_example()\n",
    "    print(f\"Category: {category}, Line: {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
