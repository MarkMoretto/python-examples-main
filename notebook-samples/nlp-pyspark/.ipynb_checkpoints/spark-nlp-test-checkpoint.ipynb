{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassifierDL for Multi-class Text Classification\n",
    "\n",
    "[link](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/classification/ClassifierDL_Train_multi_class_news_category_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_191\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_191-b12)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "java8_path: str = \"C:\\Java\\jdk1.8.0_191\"\n",
    "\n",
    "environ[\"JAVA_HOME\"] =  java8_path\n",
    "environ[\"PATH\"] = environ[\"JAVA_HOME\"] + \"/bin;\" + environ[\"PATH\"]\n",
    "\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.1\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "import sparknlp\n",
    "\n",
    "sparky = (SparkSession.builder\n",
    "            .appName(\"Spark NLP\")\n",
    "            .master(\"local[4]\")\n",
    "            .config(\"spark.driver.memory\", \"166\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "            .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.1\")\n",
    "            .getOrCreate()\n",
    "           )\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", sparky.version)\n",
    "\n",
    "# --- Old, nonworking version -- #\n",
    "# import sparknlp\n",
    "# sparky = sparknlp.start(gpu=True) # Start with GPU support\n",
    "# sparky = sparknlp.start()\n",
    "# print(\"Spark NLP version: \", sparknlp.version())\n",
    "# print(\"Apache Spark version: \", sparky.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing news category dataset\n",
    "import urllib.request as ureq\n",
    "import concurrent.futures as ccf\n",
    "\n",
    "urls_and_names = [\n",
    "    {\n",
    "        \"name\": \"train\",\n",
    "        \"url\": \"https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\",\n",
    "        \"localpath\": r\"data\\news_category_train.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"test\",\n",
    "        \"url\": \"https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv\",\n",
    "        \"localpath\": r\"data\\news_category_test.csv\",\n",
    "    }    \n",
    "]\n",
    "\n",
    "\n",
    "# Helper function.\n",
    "def rerievedata(url, fn):\n",
    "    ureq.urlretrieve(url, filename = fn)\n",
    "\n",
    "with ccf.ThreadPoolExecutor(max_workers = 4) as executor:\n",
    "    [executor.submit(rerievedata, i[\"url\"], i[\"localpath\"]) for i in urls_and_names]\n",
    "\n",
    "# Alternative on Linux\n",
    "# !wget -O data\\news_category_train.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\n",
    "# !wget -O data\\news_category_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category,description\n",
      "Business,\" Short sellers, Wall Street's dwindling band of ultra cynics, are seeing green again.\"\n",
      "Business,\" Private investment firm Carlyle Group, which has a reputation for making well timed and occasionally controversial plays in the defense industry, has quietly placed its bets on another part of the market.\"\n",
      "Business, Soaring crude prices plus worries about the economy and the outlook for earnings are expected to hang over the stock market next week during the depth of the summer doldrums.\n",
      "Business,\" Authorities have halted oil export flows from the main pipeline in southern Iraq after intelligence showed a rebel militia could strike infrastructure, an oil official said on Saturday.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View sample of data\n",
    "from sys import platform\n",
    "\n",
    "if \"win\" in platform:\n",
    "    from IPython.core.magic import register_line_magic\n",
    "\n",
    "    @register_line_magic\n",
    "    def header(line):\n",
    "        \"\"\"IPython magic method to read first N lines of a file if\n",
    "        using Windows platform.\n",
    "        \n",
    "        Example Usage\n",
    "        -------------\n",
    "        %header data\\news_category_train.csv\n",
    "        %header data\\news_category_train.csv 5\n",
    "        \"\"\"\n",
    "        lines = line.split(\" \")\n",
    "        filepath = lines[0]\n",
    "        \n",
    "        # Set number of lines variable default and update\n",
    "        # if valid data present.\n",
    "        n_lines: int = 10\n",
    "        if len(lines) == 2:\n",
    "            n_lines = int(lines[1])\n",
    "\n",
    "        output: list = []\n",
    "        with open(filepath) as f:\n",
    "            # first N lines\n",
    "            for _ in range(n_lines):\n",
    "                output.append(f.readline())\n",
    "        print(\"\".join(output))\n",
    "\n",
    "    %header data\\news_category_train.csv 5\n",
    "    del header\n",
    "\n",
    "else:\n",
    "    !head data\\news_category_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark dataframe with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data local path into variable and pass to Spark chain.\n",
    "train_path = [i[\"localpath\"] for i in urls_and_names if i[\"name\"]==\"train\"][0]\n",
    "\n",
    "# trainData = sparky.read.option(\"header\", True).csv(train_path)\n",
    "trainDF = sparky.read.csv(train_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark dataframe row count: 120,000\n",
      "\n",
      "DataFrame metadata:\n",
      "\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|category|         description|\n",
      "+--------+--------------------+\n",
      "|Business| Short sellers, W...|\n",
      "|Business| Private investme...|\n",
      "|Business| Soaring crude pr...|\n",
      "|Business| Authorities have...|\n",
      "|Business| Tearaway world o...|\n",
      "|Business| Stocks ended sli...|\n",
      "|Business| Assets of the na...|\n",
      "|Business| Retail sales bou...|\n",
      "|Business|\" After earning a...|\n",
      "|Business| Short sellers, W...|\n",
      "|Business| Soaring crude pr...|\n",
      "|Business| OPEC can do noth...|\n",
      "|Business| Non OPEC oil exp...|\n",
      "|Business| WASHINGTON/NEW Y...|\n",
      "|Business| The dollar tumbl...|\n",
      "|Business|If you think you ...|\n",
      "|Business|The purchasing po...|\n",
      "|Business|There is little c...|\n",
      "|Business|The US trade defi...|\n",
      "|Business|Oil giant Shell c...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data to confirm successful creation.\n",
    "print(f\"Spark dataframe row count: {trainDF.count():,}\\n\")\n",
    "print(\"DataFrame metadata:\\n\")\n",
    "trainDF.printSchema()\n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Use SQL to peruse data\n",
    "\n",
    "# This is a wrapper method to help view SQL data in Spark with simple queries.\n",
    "\n",
    "import re\n",
    "from pyspark.sql import dataframe\n",
    "\n",
    "\n",
    "SQL_PATTERN: str = r\"\"\"\n",
    "    (?:\\n|\\s)?\n",
    "    FROM\\s([a-z0-9_]+)\n",
    "    (?:\\n|\\s)?\n",
    "    \"\"\"\n",
    "\n",
    "p = re.compile(SQL_PATTERN, flags = re.I | re.X)\n",
    "\n",
    "\n",
    "def query_view_df(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        \"\"\"Args list should include a query string and pandas.DataFrame objects.\"\"\"\n",
    "        _query: str = None\n",
    "        _df = None\n",
    "        _viewname: str = \"\"\n",
    "        # Check argument count\n",
    "        if len(args) == 2:\n",
    "            # Assert data type of one argument\n",
    "            # Set variables accordingly.\n",
    "            if isinstance(args[0], str):\n",
    "                _query = args[0]\n",
    "                _df = args[1]\n",
    "            else:\n",
    "                _df = args[0]\n",
    "                _query = args[1]\n",
    "            \n",
    "            # Sanity check.\n",
    "            assert isinstance(_df, dataframe.DataFrame), \"PySpark.sql.dataframe.DataFrame expected!\"\n",
    "\n",
    "            \n",
    "            # Search for viewname within SQL query.\n",
    "            # If found, update _viewname variable.\n",
    "            # sql_pattern = r\"(?:\\n|\\s)?FROM\\s([a-z0-9_]+)(?:\\n|\\s)?\"\n",
    "            # res = re.search(sql_pattern, _query, flags=re.I)\n",
    "            res = p.search(_query)\n",
    "            if res:\n",
    "                _viewname = res.group(1)\n",
    "\n",
    "            # Create temporary view, run SQL, destroy temporary view.\n",
    "            _df.createOrReplaceTempView(_viewname)\n",
    "            spark.sql(_query).show()\n",
    "            spark.catalog.dropTempView(_viewname)\n",
    "            \n",
    "    return inner\n",
    "\n",
    "\n",
    "@query_view_df\n",
    "def run_query_df(query_string, dataframe) -> None:\n",
    "    \"\"\"Return output for Spark SQL view.\"\"\"\n",
    "    return query_string.strip(), dataframe\n",
    "    \n",
    "# trainDF.createOrReplaceTempView(\"dataview\")\n",
    "# sparky.sql(\"SELECT category FROM dataview GROUP BY category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-296e8503bf9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# type(trainDF)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# from pyspark.sql import dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Error!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: Error!"
     ]
    }
   ],
   "source": [
    "# type(trainDF)\n",
    "# from pyspark.sql import dataframe\n",
    "assert isinstance(trainDF, dataframe.DataFrame), \"Error!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|category|\n",
      "+--------+\n",
      "|   World|\n",
      "|Sci/Tech|\n",
      "|  Sports|\n",
      "|Business|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_query_df(\"SELECT category FROM dataview GROUP BY category\", trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/sparknlp/__init__.py\n",
    "from sparknlp.annotator import ClassifierDLApproach, UniversalSentenceEncoder\n",
    "# from sparknlp.transformers import *\n",
    "# from sparknlp.common import *\n",
    "from sparknlp.base import DocumentAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ = (DocumentAssembler()\n",
    "        .setInputCol(\"description\")\n",
    "        .setOutputCol(\"document\")\n",
    "       )\n",
    "\n",
    "# use_ = (UniversalSentenceEncoder.pretrained()\n",
    "#         .setInputCols(\"document\")\n",
    "#         .setOutputCol(\"sentence_embeddings\")\n",
    "#        )\n",
    "\n",
    "# clsdl_ = (ClassifierDLApproach().setInputCols(\"sentence_embeddings\")\n",
    "#           .setOutputCol(\"class\")\n",
    "#           .setLabelColumn(\"category\")\n",
    "#           .setMaxEpochs(5)\n",
    "#           .setEnableOutputLogs(True)\n",
    "#          )\n",
    "\n",
    "# pipeline = Pipeline(\n",
    "#     stages = [\n",
    "#         doc_,\n",
    "#         use_,\n",
    "#         clsdl_\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
