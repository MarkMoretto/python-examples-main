{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassifierDL for Multi-class Text Classification\n",
    "\n",
    "[link](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/classification/ClassifierDL_Train_multi_class_news_category_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_191\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_191-b12)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "java8_path: str = \"C:\\Java\\jdk1.8.0_191\"\n",
    "\n",
    "environ[\"JAVA_HOME\"] =  java8_path\n",
    "environ[\"PATH\"] = environ[\"JAVA_HOME\"] + \"/bin;\" + environ[\"PATH\"]\n",
    "\n",
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.1.1\n",
      "Apache Spark version:  3.1.2\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library --- #\n",
    "import re\n",
    "import urllib.request as ureq\n",
    "from functools import partial\n",
    "import concurrent.futures as ccf\n",
    "\n",
    "\n",
    "#  --- Third-party libraries --- #\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession, dataframe\n",
    "\n",
    "# https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/sparknlp/__init__.py\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import (\n",
    "    ClassifierDLApproach,\n",
    "    UniversalSentenceEncoder,\n",
    ")\n",
    "# from sparknlp.transformers import *\n",
    "# from sparknlp.common import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Create Spark Session</h1>\n",
    "<h4 style=\"text-align: center;color: white;\">Set attributes, as appropriate.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparky = (SparkSession.builder\n",
    "            .appName(\"Spark NLP\")\n",
    "            .master(\"local[4]\")\n",
    "            .config(\"spark.driver.memory\", \"166\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "            .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.1\")\n",
    "            .getOrCreate()\n",
    "           )\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", sparky.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing news category dataset\n",
    "urls_and_names = [\n",
    "    {\n",
    "        \"name\": \"train\",\n",
    "        \"url\": \"https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\",\n",
    "        \"localpath\": r\"data\\news_category_train.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"test\",\n",
    "        \"url\": \"https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv\",\n",
    "        \"localpath\": r\"data\\news_category_test.csv\",\n",
    "    }    \n",
    "]\n",
    "\n",
    "\n",
    "# Partial function for data retrieval.\n",
    "retriever = partial(ureq.urlretrieve, url = None, filename = None)\n",
    "\n",
    "# Get that data.\n",
    "with ccf.ThreadPoolExecutor(max_workers = 4) as executor:\n",
    "    [executor.submit(retriever, i[\"url\"], i[\"localpath\"]) for i in urls_and_names]\n",
    "\n",
    "# Alternative on Linux\n",
    "# !wget -O data\\news_category_train.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\n",
    "# !wget -O data\\news_category_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create IPython magic function for viewing data sample --- #\n",
    "\n",
    "# View sample of data\n",
    "from sys import platform\n",
    "\n",
    "if \"win\" in platform:\n",
    "    from IPython.core.magic import register_line_magic\n",
    "\n",
    "    @register_line_magic\n",
    "    def header(line):\n",
    "        \"\"\"IPython magic method to read first N lines of a file if\n",
    "        using Windows platform.\n",
    "        \n",
    "        Example Usage\n",
    "        -------------\n",
    "        %header data\\news_category_train.csv\n",
    "        %header data\\news_category_train.csv 5\n",
    "        \"\"\"\n",
    "        lines = line.split(\" \")\n",
    "        filepath = lines[0]\n",
    "        \n",
    "        # Set number of lines variable default and update\n",
    "        # if valid data present.\n",
    "        n_lines: int = 10\n",
    "        if len(lines) == 2:\n",
    "            n_lines = int(lines[1])\n",
    "\n",
    "        output: list = []\n",
    "        with open(filepath) as f:\n",
    "            # first N lines\n",
    "            for _ in range(n_lines):\n",
    "                output.append(f.readline())\n",
    "        print(\"\".join(output))\n",
    "\n",
    "    %header data\\news_category_train.csv 5\n",
    "    del header\n",
    "\n",
    "else:\n",
    "    !head data\\news_category_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark dataframe with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data local path into variable and pass to Spark chain.\n",
    "train_path = [i[\"localpath\"] for i in urls_and_names if i[\"name\"]==\"train\"][0]\n",
    "\n",
    "# trainData = sparky.read.option(\"header\", True).csv(train_path)\n",
    "trainDF = sparky.read.csv(train_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data to confirm successful creation.\n",
    "print(f\"Spark dataframe row count: {trainDF.count():,}\\n\")\n",
    "print(\"DataFrame metadata:\\n\")\n",
    "trainDF.printSchema()\n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Use SQL to peruse data\n",
    "\n",
    "# This is a wrapper method to help view SQL data in Spark with simple queries.\n",
    "SQL_PATTERN: str = r\"\"\"\n",
    "    (?:\\n|\\s)?\n",
    "    FROM\\s([a-z0-9_]+)\n",
    "    (?:\\n|\\s)?\n",
    "    \"\"\"\n",
    "\n",
    "p = re.compile(SQL_PATTERN, flags = re.I | re.X)\n",
    "\n",
    "def query_view_df(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        \"\"\"Args list should include a query string and pandas.DataFrame objects.\"\"\"\n",
    "        _query: str = None\n",
    "        _df = None\n",
    "        _viewname: str = \"\"\n",
    "        # Check argument count\n",
    "        if len(args) == 2:\n",
    "            # Assert data type of one argument\n",
    "            # Set variables accordingly.\n",
    "            if isinstance(args[0], str):\n",
    "                _query = args[0]\n",
    "                _df = args[1]\n",
    "            else:\n",
    "                _df = args[0]\n",
    "                _query = args[1]\n",
    "            \n",
    "            # Sanity check.\n",
    "            assert isinstance(_df, dataframe.DataFrame), \"PySpark.sql.dataframe.DataFrame expected!\"\n",
    "\n",
    "            \n",
    "            # Search for viewname within SQL query.\n",
    "            # If found, update _viewname variable.\n",
    "            # sql_pattern = r\"(?:\\n|\\s)?FROM\\s([a-z0-9_]+)(?:\\n|\\s)?\"\n",
    "            # res = re.search(sql_pattern, _query, flags=re.I)\n",
    "            res = p.search(_query)\n",
    "            if res:\n",
    "                _viewname = res.group(1)\n",
    "\n",
    "            # Create temporary view, run SQL, destroy temporary view.\n",
    "            _df.createOrReplaceTempView(_viewname)\n",
    "            spark.sql(_query).show()\n",
    "            spark.catalog.dropTempView(_viewname)\n",
    "            \n",
    "    return inner\n",
    "\n",
    "\n",
    "@query_view_df\n",
    "def run_query_df(query_string, dataframe) -> None:\n",
    "    \"\"\"Return output for Spark SQL view.\"\"\"\n",
    "    return query_string.strip(), dataframe\n",
    "    \n",
    "# trainDF.createOrReplaceTempView(\"dataview\")\n",
    "# sparky.sql(\"SELECT category FROM dataview GROUP BY category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(trainDF)\n",
    "# from pyspark.sql import dataframe\n",
    "assert isinstance(trainDF, dataframe.DataFrame), \"Error!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query_df(\"SELECT category FROM dataview GROUP BY category\", trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/sparknlp/__init__.py\n",
    "from sparknlp.annotator import ClassifierDLApproach, UniversalSentenceEncoder\n",
    "# from sparknlp.transformers import *\n",
    "# from sparknlp.common import *\n",
    "from sparknlp.base import DocumentAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ = (DocumentAssembler()\n",
    "        .setInputCol(\"description\")\n",
    "        .setOutputCol(\"document\")\n",
    "       )\n",
    "\n",
    "# use_ = (UniversalSentenceEncoder.pretrained()\n",
    "#         .setInputCols(\"document\")\n",
    "#         .setOutputCol(\"sentence_embeddings\")\n",
    "#        )\n",
    "\n",
    "# clsdl_ = (ClassifierDLApproach().setInputCols(\"sentence_embeddings\")\n",
    "#           .setOutputCol(\"class\")\n",
    "#           .setLabelColumn(\"category\")\n",
    "#           .setMaxEpochs(5)\n",
    "#           .setEnableOutputLogs(True)\n",
    "#          )\n",
    "\n",
    "# pipeline = Pipeline(\n",
    "#     stages = [\n",
    "#         doc_,\n",
    "#         use_,\n",
    "#         clsdl_\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
